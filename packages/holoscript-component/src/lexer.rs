//! Lexer for HoloScript using logos for fast tokenization.

use logos::Logos;

/// Token types generated by logos
#[derive(Logos, Debug, Clone, PartialEq)]
#[logos(skip r"[ \t\r\n]+")]
pub enum Token {
    // Keywords
    #[token("composition")]
    Composition,
    
    #[token("object")]
    Object,
    
    #[token("template")]
    Template,
    
    #[token("using")]
    Using,
    
    #[token("environment")]
    Environment,
    
    #[token("logic")]
    Logic,
    
    #[token("state")]
    State,
    
    #[token("action")]
    Action,
    
    #[token("animation")]
    Animation,
    
    #[token("spatial_group")]
    SpatialGroup,
    
    #[token("timeline")]
    Timeline,
    
    #[token("physics")]
    Physics,
    
    #[token("networked")]
    Networked,
    
    #[token("on")]
    On,
    
    #[token("if")]
    If,
    
    #[token("else")]
    Else,
    
    #[token("for")]
    For,
    
    #[token("while")]
    While,
    
    #[token("return")]
    Return,
    
    #[token("true")]
    True,
    
    #[token("false")]
    False,
    
    #[token("null")]
    Null,
    
    // Camera types
    #[token("perspective_camera")]
    PerspectiveCamera,
    
    #[token("orthographic_camera")]
    OrthographicCamera,
    
    // Light types
    #[token("point_light")]
    PointLight,
    
    #[token("directional_light")]
    DirectionalLight,
    
    #[token("spot_light")]
    SpotLight,
    
    #[token("ambient_light")]
    AmbientLight,
    
    // Traits (@ prefix)
    #[regex(r"@[a-zA-Z_][a-zA-Z0-9_]*", |lex| lex.slice()[1..].to_string())]
    Trait(String),
    
    // Identifiers
    #[regex(r"[a-zA-Z_][a-zA-Z0-9_]*", |lex| lex.slice().to_string())]
    Identifier(String),
    
    // Strings
    #[regex(r#""[^"]*""#, |lex| {
        let s = lex.slice();
        s[1..s.len()-1].to_string()
    })]
    String(String),
    
    #[regex(r#"'[^']*'"#, |lex| {
        let s = lex.slice();
        s[1..s.len()-1].to_string()
    })]
    SingleQuoteString(String),
    
    // Numbers
    #[regex(r"-?[0-9]+(\.[0-9]+)?", |lex| lex.slice().parse::<f64>().ok())]
    Number(f64),
    
    // Operators and punctuation
    #[token(":")]
    Colon,
    
    #[token(",")]
    Comma,
    
    #[token(".")]
    Dot,
    
    #[token("=")]
    Equals,
    
    #[token("=>")]
    Arrow,
    
    #[token("+")]
    Plus,
    
    #[token("-")]
    Minus,
    
    #[token("*")]
    Star,
    
    #[token("/")]
    Slash,
    
    #[token("%")]
    Percent,
    
    #[token("!")]
    Bang,
    
    #[token("&&")]
    And,
    
    #[token("||")]
    Or,
    
    #[token("<")]
    Lt,
    
    #[token(">")]
    Gt,
    
    #[token("<=")]
    Le,
    
    #[token(">=")]
    Ge,
    
    #[token("==")]
    Eq,
    
    #[token("!=")]
    Ne,
    
    // Brackets
    #[token("{")]
    LBrace,
    
    #[token("}")]
    RBrace,
    
    #[token("[")]
    LBracket,
    
    #[token("]")]
    RBracket,
    
    #[token("(")]
    LParen,
    
    #[token(")")]
    RParen,
    
    // Comments
    #[regex(r"//[^\n]*", logos::skip)]
    LineComment,
    
    #[regex(r"#[^\n]*", logos::skip)]
    HashComment,
    
    #[regex(r"/\*[^*]*\*+(?:[^/*][^*]*\*+)*/", logos::skip)]
    BlockComment,
}

/// Position information for error reporting
#[derive(Debug, Clone, PartialEq)]
pub struct Span {
    pub start: usize,
    pub end: usize,
}

impl Span {
    pub fn new(start: usize, end: usize) -> Self {
        Self { start, end }
    }
}

/// A token with its span
#[derive(Debug, Clone)]
pub struct SpannedToken {
    pub token: Token,
    pub span: Span,
}

/// Tokenize source code into a vector of spanned tokens
pub fn tokenize(source: &str) -> Result<Vec<SpannedToken>, Vec<(Span, String)>> {
    let lexer = Token::lexer(source);
    let mut tokens = Vec::new();
    let mut errors = Vec::new();
    
    for (token_result, span) in lexer.spanned() {
        match token_result {
            Ok(token) => {
                tokens.push(SpannedToken {
                    token,
                    span: Span::new(span.start, span.end),
                });
            }
            Err(_) => {
                let bad_slice = &source[span.start..span.end];
                errors.push((
                    Span::new(span.start, span.end),
                    format!("Unexpected token: '{}'", bad_slice),
                ));
            }
        }
    }
    
    if errors.is_empty() {
        Ok(tokens)
    } else {
        Err(errors)
    }
}

/// Get line and column from byte offset
pub fn get_line_col(source: &str, offset: usize) -> (usize, usize) {
    let mut line = 1;
    let mut col = 1;
    
    for (i, ch) in source.char_indices() {
        if i >= offset {
            break;
        }
        if ch == '\n' {
            line += 1;
            col = 1;
        } else {
            col += 1;
        }
    }
    
    (line, col)
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_basic_tokens() {
        let source = r#"object "Cube" @grabbable { geometry: "cube" }"#;
        let tokens = tokenize(source).unwrap();
        
        assert!(matches!(tokens[0].token, Token::Object));
        assert!(matches!(&tokens[1].token, Token::String(s) if s == "Cube"));
        assert!(matches!(&tokens[2].token, Token::Trait(s) if s == "grabbable"));
        assert!(matches!(tokens[3].token, Token::LBrace));
    }
    
    #[test]
    fn test_number_tokens() {
        let source = "position: [1.5, -2.0, 3]";
        let tokens = tokenize(source).unwrap();
        
        assert!(matches!(&tokens[0].token, Token::Identifier(s) if s == "position"));
        assert!(matches!(tokens[1].token, Token::Colon));
        assert!(matches!(tokens[2].token, Token::LBracket));
        assert!(matches!(&tokens[3].token, Token::Number(n) if (*n - 1.5).abs() < 0.001));
    }
    
    #[test]
    fn test_composition() {
        let source = r#"composition "Test" { environment { skybox: "gradient" } }"#;
        let tokens = tokenize(source).unwrap();
        
        assert!(matches!(tokens[0].token, Token::Composition));
        assert!(matches!(&tokens[1].token, Token::String(s) if s == "Test"));
    }
}
