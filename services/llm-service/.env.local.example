# HoloScript LLM Service Configuration

# Ollama API URL (local inference)
OLLAMA_URL=http://localhost:11434

# Model to use (must be pulled in Ollama)
# Available: mistral, llama2, neural-chat, etc.
OLLAMA_MODEL=mistral

# Server port
PORT=8000

# Session timeout (ms)
SESSION_TIMEOUT=86400000
